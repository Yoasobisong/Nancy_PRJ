{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32ae480b-d39d-4625-9081-e27c416b2d5d",
   "metadata": {},
   "source": [
    "# Automatic Differentiation with torch.autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d8f7da-8d2b-4795-aa62-8fe666d3c64c",
   "metadata": {},
   "source": [
    "Consider the simplest one-layer neural network, with input x, parameters w and b, and some loss function. It can be defined in PyTorch in the following manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "feef6252-96fe-4369-a2e1-96f265e3937c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(5)  # input tensor\n",
    "y = torch.zeros(3)  # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w)+b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a9e329-b1b0-4db3-80b8-88abd4859d5f",
   "metadata": {},
   "source": [
    "## Tensors, Functions and Computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96fac762-760c-4e00-9993-d3b672d5f2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x000002850B511450>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x000002850B511510>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Gradient function for z = {z.grad_fn}\")\n",
    "print(f\"Gradient function for loss = {loss.grad_fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1b6822-85f5-48aa-94f3-3ccde580c979",
   "metadata": {},
   "source": [
    "## Computing Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "714e9f69-c62d-42bd-965f-26e19a49ea4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1072, 0.3105, 0.3171],\n",
      "        [0.1072, 0.3105, 0.3171],\n",
      "        [0.1072, 0.3105, 0.3171],\n",
      "        [0.1072, 0.3105, 0.3171],\n",
      "        [0.1072, 0.3105, 0.3171]])\n",
      "tensor([0.1072, 0.3105, 0.3171])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1728cad-69d5-431a-81df-c4104b6aef38",
   "metadata": {},
   "source": [
    "## Disabling Gradient Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5375530a-7604-498a-8b6c-f69e8e2d6235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db8940e1-ee45-487b-8fd3-6a4ce8fecbd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50719e7-6dc3-4dbe-b5f2-0ce5a4ca9db9",
   "metadata": {},
   "source": [
    "## More on Computational Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b964e2-5542-4a0c-a434-9c03d39f9fba",
   "metadata": {},
   "source": [
    "In a forward pass, autograd does two things simultaneously:\n",
    "\n",
    "run the requested operation to compute a resulting tensor\n",
    "\n",
    "maintain the operationâ€™s gradient function in the DAG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa6ca17-92c2-4913-bb6a-8a9a2a50afe5",
   "metadata": {},
   "source": [
    "## Optional Reading: Tensor Gradients and Jacobian Products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b33768-6293-43e8-86ce-0af5bb0d6a60",
   "metadata": {},
   "source": [
    "In many cases, we have a scalar loss function, and we need to compute the gradient with respect to some parameters. However, there are cases when the output function is an arbitrary tensor. In this case, PyTorch allows you to compute so-called Jacobian product, and not the actual gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88c1ca12-2022-448d-800d-3f10cdb7b591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call\n",
      "tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.]])\n",
      "\n",
      "Second call\n",
      "tensor([[8., 4., 4., 4., 4.],\n",
      "        [4., 8., 4., 4., 4.],\n",
      "        [4., 4., 8., 4., 4.],\n",
      "        [4., 4., 4., 8., 4.]])\n",
      "\n",
      "Call after zeroing gradients\n",
      "tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.]])\n"
     ]
    }
   ],
   "source": [
    "inp = torch.eye(4, 5, requires_grad=True)\n",
    "out = (inp+1).pow(2).t()\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "print(f\"First call\\n{inp.grad}\")\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "print(f\"\\nSecond call\\n{inp.grad}\")\n",
    "inp.grad.zero_()\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "print(f\"\\nCall after zeroing gradients\\n{inp.grad}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch_env)",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
