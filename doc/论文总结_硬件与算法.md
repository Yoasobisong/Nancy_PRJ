# 两篇论文重点总结：硬件 & 算法

> **论文1**: _Human-robot facial coexpression_ (Sci. Robotics, 2024) — "Emo" 机器人  
> **论文2**: _Learning realistic lip motions for humanoid face robots_ (Sci. Robotics, 2026) — 唇部同步机器人  
> **团队**: Columbia University, Creative Machines Lab — Yuhang Hu, Hod Lipson 等

---

## 一、论文1：Human-robot Facial Coexpression（表情共表达）

### 1.1 研究目标

让机器人 **预测** 人类即将出现的面部表情（如微笑），并在人类表情出现的 **同时** 做出相同表情（称为"共表达 co-expression"），而不是延迟模仿。

### 1.2 硬件设计 — "Emo" 机器人头部

| 项目             | 详细参数                                                  |
| ---------------- | --------------------------------------------------------- |
| **自由度 (DOF)** | **26 DOF** 面部驱动                                       |
| **驱动方式**     | 26个舵机/电机直接驱动（非线缆驱动）                       |
| **面部皮肤**     | 柔性硅胶皮肤，通过 **磁性快拆连接器** 固定到机械结构上    |
| **连接方式**     | 磁铁直接贴在硅胶皮肤内侧，磁吸到机械持架上，方便拆装/更换 |
| **嘴唇设计**     | 软唇 + 被动关节 + 连杆机构，可推可拉（非仅拉）            |
| **眼睛**         | 高分辨率 RGB 摄像头嵌入眼球内部（用于目光追踪+观察人脸）  |
| **传感**         | 眼球内摄像头（视觉）+ 麦克风（听觉）                      |
| **计算平台**     | 底座内集成边缘计算设备（低延迟）                          |
| **音频**         | 内置扬声器 + 麦克风                                       |

**硬件关键创新点：**

- 26 DOF 提供高维度面部表情空间，覆盖多种表情
- 磁性快拆系统便于快速迭代设计和维护
- 推/拉双向驱动克服了传统线缆式只能拉的限制
- 摄像头在眼球内部实现自然目光接触

### 1.3 算法流程

```
┌─────────────────────────────────────────────────────────────┐
│                    训练阶段                                   │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  Step 1: 自建模 (Self-Modeling)                              │
│  ┌──────────┐     ┌──────────┐     ┌──────────────────┐    │
│  │ 机器人做  │ --> │ 摄像头录 │ --> │ 学习: 电机指令 ↔  │    │
│  │ 随机表情  │     │ 制视频   │     │ 面部外观 的映射   │    │
│  └──────────┘     └──────────┘     └──────────────────┘    │
│  （类似婴儿照镜子学习自己的脸）                                │
│                                                             │
│  Step 2: 表情预测模型训练                                     │
│  ┌──────────────┐     ┌──────────────────────┐              │
│  │ 大量人脸表情  │ --> │ 训练模型: 人脸细微变  │              │
│  │ 视频数据      │     │ 化 → 预测即将出现的   │              │
│  │              │     │ 表情 (提前839ms)      │              │
│  └──────────────┘     └──────────────────────┘              │
│                                                             │
│  Step 3: 逆运动学自模型 (Inverse Kinematic Self-Model)        │
│  ┌──────────────┐     ┌──────────────────────┐              │
│  │ 目标表情图像  │ --> │ 生成对应的电机指令    │              │
│  └──────────────┘     └──────────────────────┘              │
│                                                             │
├─────────────────────────────────────────────────────────────┤
│                    部署阶段                                   │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  眼球摄像头观察人脸                                           │
│       ↓                                                     │
│  表情预测模型: 检测人脸细微变化，预测839ms后的表情              │
│       ↓                                                     │
│  逆运动学模型: 将目标表情转换为26个电机指令                     │
│       ↓                                                     │
│  机器人与人类同步做出表情                                      │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

**算法关键点：**

- **自建模 (Self-Modeling)**：机器人先学会理解自己的脸 → 电机指令与表情外观的对应关系
- **表情预测**：通过人脸细微变化（微表情前兆）提前 ~839ms 预测出即将到来的表情
- **逆运动学**：给定目标面部图像 → 输出电机控制指令
- **两个AI模型协同**：预测模型 + 逆运动学模型

---

## 二、论文2：Learning Realistic Lip Motions（唇部同步）

### 2.1 研究目标

让仿人机器人实现 **实时、逼真的唇音同步 (Lip-Audio Sync)**，不依赖预定义动作，通过自监督学习从语音音频直接推断唇部运动轨迹。

### 2.2 硬件设计 — 唇部机构

| 项目           | 详细参数                                                |
| -------------- | ------------------------------------------------------- |
| **唇部自由度** | **10 DOF** 唇部驱动                                     |
| **嘴角**       | 2对电机（每侧2个堆叠电机），形成2D运动空间，可回缩+前突 |
| **上唇**       | 3个独立驱动，垂直方向运动，下降时自动外翻（模仿撅嘴）   |
| **下唇**       | 2个独立驱动，抬升时通过欠驱动旋转轴自动外翻             |
| **下颌**       | 1个驱动                                                 |
| **面部皮肤**   | 柔性硅胶，磁性快拆连接器固定                            |
| **连接器**     | 磁性快拆，用强力胶粘在硅胶皮肤上，可快速拆装            |
| **传感**       | 高分辨率RGB摄像头（眼球内） + 麦克风 + 扬声器           |
| **计算**       | 底座内边缘计算设备                                      |

**10 DOF 唇部分布：**

```
          ┌─── 上唇 (3 DOF) ───┐
          │ 左上  中上  右上    │
嘴角左(2DOF)                    嘴角右(2DOF)
(回缩/前突)                     (回缩/前突)
          │ 左下      右下    │
          └─── 下唇 (2 DOF) ──┘
                下颌 (1 DOF)
```

**硬件关键创新点：**

- 10 DOF 唇部机构覆盖 24个辅音 + 16个元音发音，分为 **12个视位 (viseme) 类别**
- 推/拉双向驱动，比传统仅线缆拉动有更大动作范围
- 欠驱动旋转轴让唇部运动更自然（被动顺应）
- 磁性快拆便于快速迭代皮肤设计

### 2.3 算法流程 — 自监督学习流水线

```
┌──────────────────────────────────────────────────────────────────┐
│                     数据收集阶段                                  │
├──────────────────────────────────────────────────────────────────┤
│                                                                  │
│  "Motor Babbling" (电机抖动探索)                                  │
│  ┌──────────┐     ┌──────────┐     ┌─────────────┐              │
│  │ 机器人随机│ --> │ 前置摄像 │ --> │ 数据集:     │              │
│  │ 做嘴部动作│     │ 头录制   │     │ {视频帧, 电 │              │
│  │(20000帧) │     │          │     │ 机指令}对   │              │
│  └──────────┘     └──────────┘     └─────────────┘              │
│                                                                  │
├──────────────────────────────────────────────────────────────────┤
│                     训练阶段                                      │
├──────────────────────────────────────────────────────────────────┤
│                                                                  │
│  ① VAE (变分自编码器) 训练                                        │
│  ┌──────────────────────────────────────────────┐                │
│  │  输入: 机器人真实视频帧(20000) + 合成视频帧    │                │
│  │  ↓                                           │                │
│  │  Encoder → 隐空间向量(16维) → Decoder         │                │
│  │  ↓                                           │                │
│  │  目的: 将真实图像和合成图像映射到同一隐空间     │                │
│  │  损失: MSE重建损失 + KL散度正则化              │                │
│  └──────────────────────────────────────────────┘                │
│                                                                  │
│  ② 隐空间匹配 → 生成语音相关电机指令                               │
│  ┌──────────────────────────────────────────────┐                │
│  │  合成视频帧 → VAE Encoder → 隐向量 L't         │                │
│  │  真实视频帧 → VAE Encoder → 隐向量 Li          │                │
│  │  找最近邻: L* = argmin ||L't - Li||₂           │                │
│  │  取对应电机指令 A* 作为语音相关指令              │                │
│  │  迭代4次优化                                   │                │
│  └──────────────────────────────────────────────┘                │
│                                                                  │
│  ③ FAT (Facial Action Transformer) 训练                          │
│  ┌──────────────────────────────────────────────┐                │
│  │  Transformer Encoder:                        │                │
│  │    输入: 前2帧电机指令 (At-2, At-1)            │                │
│  │    + 位置编码                                 │                │
│  │    → 编码时序依赖关系                          │                │
│  │                                              │                │
│  │  Transformer Decoder:                        │                │
│  │    输入: VAE隐向量 (Lt, Lt+1)                  │                │
│  │    → 预测未来电机指令 (At, At+1)               │                │
│  │                                              │                │
│  │  损失函数:                                    │                │
│  │    L_total = L_MAE + λ * L_closure            │                │
│  │    MAE: 预测vs目标的平均绝对误差               │                │
│  │    Closure: 闭唇惩罚(b/p/m发音需完全闭合)      │                │
│  │  数据集: 40000帧 (20000原始 + 20000翻转增强)   │                │
│  └──────────────────────────────────────────────┘                │
│                                                                  │
├──────────────────────────────────────────────────────────────────┤
│                     部署阶段                                      │
├──────────────────────────────────────────────────────────────────┤
│                                                                  │
│  文本(ChatGPT生成)                                                │
│    ↓                                                             │
│  TTS (文字→语音)                                                  │
│    ↓                                                             │
│  Wav2Lip (语音+人脸 → 合成口型视频)                                │
│    ↓                                                             │
│  VAE Encoder (合成视频帧 → 隐向量L't)                              │
│    ↓                                                             │
│  FAT (隐向量 → 平滑电机指令)                                      │
│    ↓                                                             │
│  电机执行（实时唇部运动）                                          │
│                                                                  │
└──────────────────────────────────────────────────────────────────┘
```

**算法关键点：**

- **自监督学习**：无需人工标注，机器人自主探索（Motor Babbling）建立数据集
- **VAE**：桥接"合成视频"和"真实机器人"之间的域差异（16维隐空间）
- **Wav2Lip**：给定音频 → 生成面部口型合成视频（作为目标参考）
- **FAT**：Transformer 架构确保时序一致性，避免抖动
- **Closure Loss**：专门针对双唇音 (b/p/m) 的闭合准确性
- **多语言泛化**：仅用英语训练，成功泛化到 11 种语言

---

## 三、两篇论文关系 & 技术演进

```
论文1 (2024): Emo - 表情共表达
  ├── 26 DOF 全脸
  ├── 自建模 (Self-Modeling)
  ├── 表情预测 (~839ms 提前量)
  └── 逆运动学自模型
        │
        ▼ 技术演进
论文2 (2026): 唇部同步
  ├── 10 DOF 专注于嘴唇 (更精细)
  ├── Motor Babbling + VAE (自监督)
  ├── Wav2Lip 合成目标
  ├── FAT Transformer (时序平滑)
  └── 多语言泛化 (11种语言)
```

**演进关键：**

- 论文1 关注 **全脸表情**（26 DOF），论文2 聚焦 **嘴唇精细运动**（10 DOF 唇部）
- 论文1 用"自建模 + 逆运动学"，论文2 改进为"VAE + Transformer"，更端到端
- 两者都用 **磁性快拆硅胶皮肤**，**推拉双向驱动**，来自同一硬件平台迭代

---

## 四、如果你要复现/DIY，关键要素清单

### 4.1 硬件部分

| 模块          | 建议方案                                                              | 参考           |
| ------------- | --------------------------------------------------------------------- | -------------- |
| **舵机/电机** | 高精度数字舵机 (如 Dynamixel / Feetech SCS系列)，需要至少10个用于唇部 | 论文用伺服电机 |
| **唇部机构**  | 连杆 + 磁性连接器，上/下/角分区独立控制                               | 10 DOF 设计    |
| **面部皮肤**  | 铂金硫化硅胶 (如 Ecoflex / Dragon Skin)，翻模制作                     | 柔性硅胶       |
| **快拆连接**  | 小型圆柱钕磁铁 (Ø3-5mm)，强力胶粘在硅胶内侧                           | 磁性快拆       |
| **摄像头**    | 小型高分辨率USB摄像头（可嵌入眼球），或用外部摄像头替代               | RGB cameras    |
| **麦克风**    | MEMS 麦克风或 USB 麦克风                                              | 语音输入       |
| **扬声器**    | 小型全频扬声器                                                        | 语音输出       |
| **控制板**    | STM32/ESP32 做底层舵机控制 + Jetson Nano/Orin 做AI推理                | 边缘计算       |
| **结构件**    | 3D打印（FDM/SLA），连杆、支架、眼球座                                 | 机械结构       |

### 4.2 算法部分

| 步骤         | 技术方案                          | 说明                          |
| ------------ | --------------------------------- | ----------------------------- |
| **TTS**      | Edge-TTS / VITS / ChatTTS         | 文字→语音                     |
| **Wav2Lip**  | Wav2Lip (开源)                    | 根据音频生成嘴型合成视频      |
| **VAE**      | PyTorch 实现 Conv VAE             | 16维隐空间，编码面部状态      |
| **FAT**      | PyTorch Transformer               | Encoder-Decoder，预测电机指令 |
| **数据收集** | Motor Babbling 自动采集           | 随机电机指令 + 录像 → 20000帧 |
| **表情预测** | CNN/Transformer + 人脸视频        | 提前预测表情（如果做共表达）  |
| **部署推理** | ONNX Runtime / TensorRT on Jetson | 实时推理                      |

### 4.3 软件工具链

```
Python 3.10+
├── PyTorch (模型训练)
├── OpenCV (图像处理)
├── MediaPipe (人脸关键点检测)
├── Wav2Lip (唇部合成)
├── NumPy / SciPy (数据处理)
├── Edge-TTS / VITS (文字转语音)
└── Serial/I2C/PWM (舵机通信)
```

### 4.4 开源资源

- **论文2代码和模型**: https://doi.org/10.5281/zenodo.17804235
- **论文2数据集**: https://doi.org/10.5061/dryad.j6q573nrc
- **Wav2Lip**: https://github.com/Rudrabha/Wav2Lip
- **MediaPipe**: https://github.com/google/mediapipe

---

## 五、简化实现思路（MVP 版本）

如果不追求论文的完整复杂度，可以分阶段实现：

### Phase 1：硬件基础 + 简单唇动

- 3D打印头部骨架
- 3~5个舵机控制基础唇部动作（上唇/下唇/下颌）
- 硅胶翻模面部皮肤
- 音量幅度驱动下颌开合（amplitude baseline）

### Phase 2：升级到 VAE + 精细唇动

- 增加到 10 DOF 唇部
- 实现 Motor Babbling 数据采集
- 训练 VAE 映射视觉→电机空间
- 对接 Wav2Lip 生成目标口型

### Phase 3：Transformer 时序平滑 + 表情

- 训练 FAT 模型
- 实现多语言支持
- 增加眼部/眉毛表情控制
- 实现实时对话（LLM + TTS + 唇同步）
