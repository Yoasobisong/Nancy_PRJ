# Nancy 机器人 — 强化学习路线图

> 目标：4 周内掌握所有必要知识，能读懂 Columbia Emo 论文并独立实现  
> 前提：已有嵌入式基础、Python 基础、吴恩达课程前半部分  
> 强度：每天 4-6 小时学习

---

## 知识全景

```
Nancy 项目 = 4 大模块

① 深度学习基础        看懂论文、理解模型、会训练
② FACS + 面部感知     项目的"语言"，贯穿软硬件
③ 机械结构设计        连杆、运动学、CAD、硅胶皮肤
④ 系统整合            Jetson + 舵机 + ROS + 实时控制
```

---

## 第 1 周：深度学习基石与框架过渡（Coursera C4+C5 → PyTorch）

> **设计思路补充**：作为底层/嵌入式开发者，你的学习不能走马观花。吴恩达的 Coursera 专项课程（C4 卷积神经网络、C5 序列模型）中包含极具价值的底层作业。与其单纯看视频，不如挑出**最核心的几周作业**认真跑通。学完底层理论后，再于周末平滑切换至工业部署首选的 PyTorch 框架。

### Day 1：CNN 底层逻辑（吴恩达 C4 Week 1）

| 时间 | 内容                                           | 核心输出/作业                                                                                     |
| ---- | ---------------------------------------------- | ------------------------------------------------------------------------------------------------- |
| 上午 | 理论：卷积层、步长、Padding、池化层            | [YouTube Playlist: CNN](https://www.youtube.com/playlist?list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF) |
| 下午 | 理论：为什么要用卷机网络而不是全连接？         | 同上                                                                                              |
| 晚上 | **实战**：[已完成✅] Numpy 手推卷积前/反向传播 | Coursera 作业: `Convolution_model_Step_by_Step_v2a.ipynb`                                         |

**里程碑**：理解由图像张量进入，经过滤波器提取出特征图的数学本质。

### Day 2：经典视觉架构与其底层思想（吴恩达 C4 Week 2）

| 时间 | 内容                               | 核心输出/作业                                                               |
| ---- | ---------------------------------- | --------------------------------------------------------------------------- |
| 上午 | 理论：LeNet-5, AlexNet, VGG-16     | 感受感受野和深度的作用                                                      |
| 下午 | 理论：ResNet（残差网络）、1x1 卷积 | 明白为什么残差连接（Add）能防止梯度消失，让网络能做得很深                   |
| 晚上 | **实战**：用 Keras 搭建基础 CNN    | Coursera 作业: `Convolution_model_Application`（体验一下高级 API 的积木感） |

_(注：C4 的 Week 3 目标检测 YOLO、Week 4 人脸识别，由于 Nancy 项目主要依赖 MediaPipe 提取特征，这两周可暂时跳过，节省时间)_

### Day 3：机器人的时间轴记忆（吴恩达 C5 Week 1）

| 时间 | 内容                                   | 核心输出/作业                                                                                                 |
| ---- | -------------------------------------- | ------------------------------------------------------------------------------------------------------------- |
| 上午 | 理论：为什么要用序列模型？RNN 基础架构 | [YouTube Playlist: Sequence Models](https://www.youtube.com/playlist?list=PLkDaE6sCZn6F6wUI9tvS_Gw1vaFAx6rd6) |
| 下午 | 理论：GRU 与 LSTM 的内部逻辑门         | 重点理解 C（记忆细胞）和 H（隐藏状态）是如何随时间传递的                                                      |
| 晚上 | **在纸上画图**：厘清时间序列的数据维度 | `[Batch_Size, Time_Steps, Feature_Dim]` ← 嵌入式时序部署必须建立的维度直觉                                    |

### Day 4：现代序列模型基石（吴恩达 C5 Week 3）

| 时间 | 内容                                       | 核心输出/作业                                                |
| ---- | ------------------------------------------ | ------------------------------------------------------------ |
| 上午 | 理论：Encoder-Decoder（编码器-解码器）架构 | 这是如何把一段视频特征，浓缩后转化为一段伺服指令的核心架构   |
| 下午 | 理论：Attention 机制（注意力模型）的直觉   | 懂它为什么能解决长时遗忘的问题                               |
| 晚上 | 回顾 Nancy 论文：画出系统级网络数据流向    | 论文结构：人类动作特征输入 -> 网络处理 -> 机械手动作指令输出 |

### Day 5：武器库切换——PyTorch 基础（张量与求导）

_此时你已通过 Coursera 掌握了理论和参数概念，现在正式全面转向 PyTorch_

| 时间 | 内容                                        | 核心输出/作业                                                               |
| ---- | ------------------------------------------- | --------------------------------------------------------------------------- |
| 上午 | Tensor 运算与 CPU/GPU (`.cuda()`) 内存流向  | [小土堆 PyTorch 教程](https://www.bilibili.com/video/BV1hE411t7RN) 前端部分 |
| 下午 | 核心引擎：Autograd（自动求导）、nn.Module   | 官方文档或教程，对比 PyTorch 自动算梯度和 Day1 你手写 Numpy 梯度的区别      |
| 晚上 | 动手：PyTorch 搭建双层全连接网络(MLP)作回归 | 跑通“定义网络 - 清空梯度 - 反向传播 - 更新权重”的训练闭环                   |

### Day 6：PyTorch 数据流与视觉实战

| 时间 | 内容                                      | 核心输出/作业                                                 |
| ---- | ----------------------------------------- | ------------------------------------------------------------- |
| 上午 | 数据管道：自定义 Dataset 和 DataLoader    | 学习如何把硬盘上的假数据打包成 Batch 送进网络                 |
| 下午 | 使用 `nn.Conv2d` 与 `nn.MaxPool2d`搭网络  | 把 C4 学到的理论变成 PyTorch 里的实际图纸                     |
| 晚上 | 动手：跑通 MNIST 或 CIFAR 10 标准分类流程 | 完成一次标准化的端到端训练迭代记录 (Loss 下降，Accuracy 提升) |

### Day 7：终极实战模拟（时间序列回归）

_结合 C5 序列理论和 PyTorch，写一段面向南希机器人的序列逻辑代码_

| 时间 | 内容                                            |
| ---- | ----------------------------------------------- |
| 上午 | 调用 PyTorch 的 `nn.LSTM` 模块                  |
| 下午 | 编写面部时序数据的 DataLoader（滑动窗口提取法） |
| 晚上 | **动手**：写一个模拟 Nancy 的网络训练骨架！     |

**必须理解的骨架范例（结合了 CNN / LSTM）：**

```python
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

# 定义面向 Nancy (136 维面部特征序列 -> 24 维舵机指令序列) 的网络模型
class SequenceRegressionModel(nn.Module):
    def __init__(self, input_dim=136, output_dim=24, hidden_dim=64):
        super().__init__()
        # batch_first=True 非常重要，这样输入张量就是 [Batch, Time_Steps, Feature]
        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        # x.shape 进来的样子: [Batch, Time, Input_Dim]
        out, (h_n, c_n) = self.lstm(x)
        # out.shape 生成的样子: [Batch, Time, Hidden_Dim]
        predictions = self.fc(out)
        # predictions 预测的最终序列: [Batch, Time, Output_Dim] -> 直接映射给舵机数组
        return predictions

# 训练闭环概念测试：
model = SequenceRegressionModel()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
loss_fn = nn.MSELoss()

# 假装你的 dataloader 里吐出的是 [批量大小, 帧数, 特征数] 的张量
dummy_input = torch.randn(8, 128, 136)  # 8 个片段, 每个片段包含连续 128 帧画面, 每帧 136 个追踪点
dummy_target = torch.randn(8, 128, 24)  # 对应的 128 帧里 24 个舵机目标角度

for epoch in range(10):  # 走10个epoch看看梯度能不能正常下降
    pred = model(dummy_input)
    loss = loss_fn(pred, dummy_target)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    print(f"Epoch {epoch}, Loss: {loss.item():.4f}")
```

---

## 第 2 周：顶刊论文拆解与面部追踪实战

> **调研修正**：Columbia Emo 机器人的核心不是简单的“看到表情再做表情”，而是**预测（Anticipating）**，能提前 840 毫秒预测人类情绪；并且它的运动模型是通过**对镜自我学习（Self-modeling）**建立的逆运动学网络。这周以此为目标重新打造。

### Day 8：精读 Columbia Emo 核心论文

| 时间 | 内容                                                | 资源与任务                                                                                                      |
| ---- | --------------------------------------------------- | --------------------------------------------------------------------------------------------------------------- |
| 上午 | 研读 _Science Robotics_ 发表的 Emo 论文背景与架构   | [Columbia Emo 论文主页](https://www.creativemachineslab.com/emo.html)                                           |
| 下午 | 重点钻研 1：**预测网络（Anticipation Network）**    | 弄懂 AI 如何抓住面部的微表情，提前 840ms 做出响应。输入是前置视频帧，输出是未来的情绪分布。                     |
| 晚上 | 重点钻研 2：**对镜自学习（Self-Modeling Network）** | 弄懂 Emo 如何看着镜子胡乱动舵机，从而自己学会“哪个舵机拉扯会导致哪里的硅胶运动”（这省去了复杂的物理数学建模）。 |

### Day 9：FACS 面部动作编码系统

| 时间 | 内容                                       | 资源                                                                        |
| ---- | ------------------------------------------ | --------------------------------------------------------------------------- |
| 上午 | FACS 基础：什么是 AU（Action Unit）及强度  | [FACS Wikipedia](https://en.wikipedia.org/wiki/Facial_Action_Coding_System) |
| 下午 | 熟悉核心 AU：皱眉(AU4)、微笑(AU12)、睁眼等 | 了解人类表皮肌肉是如何收缩引起表皮形变的，这是控制软体硅胶脸的基础。        |
| 晚上 | 实践：对着镜子，把自己的脸当成机器人       | 体验一下你的表情是由多少块面部肌肉协同拉扯（协同舵机）完成的。              |

### Day 10-11：面部关键点与微表情提取

| 时间 | 内容                                       | 资源                                                                                     |
| ---- | ------------------------------------------ | ---------------------------------------------------------------------------------------- |
| 上午 | MediaPipe Face Mesh 安装与测试             | Emo 机器人是在眼睛（瞳孔）里安装了高分辨率摄像头来捕捉人类表情。你也用电脑摄像头起步。   |
| 下午 | 理解 468 个关键点的三维空间（XYZ）分布     | [Google MediaPipe 官方文档](https://google.github.io/mediapipe/solutions/face_mesh.html) |
| 晚上 | 写代码：提取瞳孔间距纠正尺度，计算关键参数 | 提取嘴角的拉伸速度追踪微笑趋势，提取眼轮匝肌距追踪皱眉趋势。                             |

### Day 12：建立你自己的表情捕捉数据管道

| 时间 | 内容                                                                   |
| ---- | ---------------------------------------------------------------------- |
| 上午 | 使用 OpenCV 录制你自己的脸部表情（模拟人类互动数据集）                 |
| 下午 | 写 Python 脚本自动跑批你的视频，抽取出连续时间帧（如 30fps）的特征张量 |
| 晚上 | 将提取的数据打包保存为 `.npy` 矩阵文件，为下周深度学习做准备           |

---

## 第 3 周：硬核机械与电子系统（双核机构复现）

> **结构修正**：结合两篇论文，机器人的核心硬件具有极大挑战：**26 个高密度 DOF**（其中 10 个专注唇部），且突破了传统纯拉线的限制，实现了**推/拉双向驱动**与**欠驱动顺应关节**；皮肤采用**磁性快拆结构**。这周是你的主战场！

### Day 13-14：10 DOF 仿生唇部与推拉双向机构设计

| 时间 | 内容                               | 资源与任务                                                                                               |
| ---- | ---------------------------------- | -------------------------------------------------------------------------------------------------------- |
| 上午 | 研读论文的硬件补充材料 (Hardware)  | 重点看 10-DOF 口部结构：左右嘴角(各2DOF，控制前突/回缩)、上唇(3DOF，下降外翻)、下唇(2DOF，抬升外翻)。    |
| 下午 | **双向推/拉（Push-Pull）机构分析** | 传统线缆只能拉。你需要设计刚性/半刚性连杆结合柔性枢纽，使得电机既能把嘴唇扯开，也能撅起来（Puckering）。 |
| 晚上 | Fusion 360 建模实战                | 设计一个具有磁铁锚点（Magnetic quick-release connectors）的唇部独立驱动单元雏形，准备 3D 打印测试。      |

### Day 15：眼球深度的秘密与高频总线通信

| 时间 | 内容                                                                                                                                      |
| ---- | ----------------------------------------------------------------------------------------------------------------------------------------- |
| 上午 | **眼部硬件视角**：设计一个 2 自由度眼球云台，中心必须留通孔放置微型广角摄像头。这是实现目光接触（Eye Contact）的绝对前提。                |
| 下午 | 思考 26 个微型舵机的布局（空间几何折叠）与安装背板设计。                                                                                  |
| 晚上 | **电子总线提速**：研究双 PCA9685 级联架构。修改 Jetson I2C 波特率（提权至 400kHz 或更高级别），确保 26 个通道的高频同步刷新没有任何抖动。 |

### Day 16-17：硅胶皮肤翻模与"磁吸快拆"工艺

| 时间 | 内容                                                                                             |
| ---- | ------------------------------------------------------------------------------------------------ |
| 上午 | 学习铂金硅胶（如 Ecoflex/Dragon Skin）的调配与真空除泡工艺。                                     |
| 下午 | **锚点植入工艺**：如何在硅胶固化前，将微型钕磁铁（Ø3-5mm）精准埋入对应脸上 26 个肌肉节点的位置？ |
| 晚上 | 组装你的第一个测试单元：电机 + 连杆 + 磁吸扣 + 单块硅胶，并跑通第一波平滑缓入缓出代码。          |

### Day 18：Motor Babbling（电机自主探索）数据采集

| 时间 | 内容                                                                                                                                                                                                                  |
| ---- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 全天 | **极度重要的数据集建立**：写一段脚本，让组装好的面部系统进行 "Motor Babbling"（随机胡乱发送舵机组合指令），同时用前置摄像头录下形变后的脸。生成至少数千帧 `[视频帧, 电机指令]` 的配对矩阵。这是下周自监督学习的命脉。 |

---

## 第 4 周：前沿双模型融合（视觉共情 + 音频唇语）

> **冲刺目标**：在边缘计算端（Jetson），融合 **Paper 1 的视觉表情预测（共表达）** 与 **Paper 2 的音频驱动唇步同步（VAE+FAT）**，打造终极拟人内核。

### Day 19-20：管线 A —— 视觉表情共表达（Anticipation & Inverse Kinematics）

| 时间   | 内容                                                                                                                                                                               |
| ------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Day 19 | **逆运动学自模型训练**：拿 Day 18 录制的 Motor Babbling 数据，训练一个极简的特征解码器。输入是想要的表情特征，输出是 26 个舵机的目标拉力值。这一步彻底跳过复杂的物理建模。         |
| Day 20 | **意图预测 (Anticipation) 网络搭建**：复现论文核心因果推理模型。从人类对话视频中，训练网络通过人脸微表情，提前 **840 毫秒** 预测出目标表情（比如察觉到别人要笑了，提取发笑趋势）。 |

### Day 21-23：管线 B —— 逼真唇部同步（Learn Realistic Lip Motions）

| 时间   | 内容                                                                                                                                                                                                |
| ------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Day 21 | **理解 Wav2Lip 与 VAE 域自适应**：了解如何利用开源的 Wav2Lip 将音频转为 2D 目标合成图像。然后使用 PyTorch 搭建一个变分自编码器 (VAE)，把摄像头真实的机器人脸和合成脸，映射到同一个 **16维隐空间**。 |
| Day 22 | **面部动作 Transformer (FAT) 建模**：唇同步不能发生抖动。用 Transformer Encoder-Decoder 结构，输入前一帧的电机动作和当前隐空间向量，输出极致平滑的当前电机指令。                                    |
| Day 23 | **加闭唇惩罚 (Closure Loss)**：重点攻克 "b/p/m" 这类爆破双唇音。修改损失函数，当遇到双唇音特征时，强制要求上下唇的机械距必须为零合拢（这是避免"假唱感"的核心秘密）。                                |

### Day 24-26：系统整合与端侧部署 (Jetson)

| 时间   | 内容                                                                                                                                                                                     |
| ------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Day 24 | **大一统并发架构设计**：<br>- 线程 1：麦克风监听 -> TTS/Wav2Lip推理 -> VAE+FAT -> 唇部 10 舵机指令<br>- 线程 2：眼部摄像头感知 -> Anticipation预测 -> 逆运动网络 -> 脸部其他 16 舵机指令 |
| Day 25 | **推理加速部署 (TensorRT)**：将庞大的 Transformer 和预测网络分别转为 ONNX 并由 Jetson 的 GPU 加速编译为 TensorRT 引擎，压榨极致延迟保证 840ms 半秒级的身心合一。                         |
| Day 26 | **实机对话闭环**：接入 LLM（如 ChatGPT API），进行全流程对话测试：语音打招呼 -> LLM思考生成文本 -> Edge-TTS 转语音 -> Robot 并发进行脸部微表情调整及同步唇语发音！                       |

### Day 27-28：技术复盘与迭代

- **验收测试**：测试 11 种多语言的唇部泛化能力（论文证明仅用英语训练也能泛化到中/日/法等）。
- **难点梳理**：系统录制演示视频。总结硅胶弹性的热衰减、Wav2Lip 推理延迟带来的滞后等问题。
- **展望规划**：规划下一代底层硬件系统及多模态端到端模型的预研。

---

## 资源汇总

### 视频课程

| 资源            | 内容               | 时长 | 链接                                                                                         |
| --------------- | ------------------ | ---- | -------------------------------------------------------------------------------------------- |
| 吴恩达 CNN      | 卷积神经网络       | ~10h | [YouTube Playlist](https://www.youtube.com/playlist?list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF) |
| 吴恩达 序列模型 | RNN/LSTM/Attention | ~10h | [YouTube Playlist](https://www.youtube.com/playlist?list=PLkDaE6sCZn6F6wUI9tvS_Gw1vaFAx6rd6) |
| 小土堆 PyTorch  | PyTorch 入门       | ~5h  | [B 站](https://www.bilibili.com/video/BV1hE411t7RN)                                          |
| 3Blue1Brown     | 神经网络可视化     | ~1h  | [YouTube](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)          |
| FACS 教程       | 面部动作编码       | ~2h  | YouTube 搜 "FACS tutorial"                                                                   |

### 工具 & 库

| 工具              | 用途             | 安装                                                                      |
| ----------------- | ---------------- | ------------------------------------------------------------------------- |
| PyTorch           | 深度学习框架     | `pip install torch torchvision`                                           |
| MediaPipe         | 面部关键点检测   | `pip install mediapipe`                                                   |
| OpenFace          | AU 检测 (备选)   | [GitHub Releases](https://github.com/TadasBaltrusaitis/OpenFace/releases) |
| OpenCV            | 图像处理         | `pip install opencv-python`                                               |
| adafruit-servokit | PCA9685 舵机控制 | `pip install adafruit-circuitpython-servokit`                             |
| Fusion360         | CAD 建模         | [免费下载](https://www.autodesk.com/products/fusion-360/personal)         |

### 论文 & 参考

| 资源                                                                                 | 说明            |
| ------------------------------------------------------------------------------------ | --------------- |
| [Columbia Emo Robot](https://www.creativemachineslab.com/emo.html)                   | 核心论文 + 视频 |
| [FACS Manual (Wikipedia)](https://en.wikipedia.org/wiki/Facial_Action_Coding_System) | AU 编码参考     |
| [OpenFace 2.0 论文](https://ieeexplore.ieee.org/document/8373812)                    | AU 检测算法     |
| [MediaPipe Face Mesh](https://google.github.io/mediapipe/solutions/face_mesh.html)   | 468 关键点文档  |

---

## 每日检查清单

```
□ 今天学了什么新概念？能用自己的话解释吗？
□ 今天写了代码吗？能跑起来吗？
□ 这个知识和 Nancy 项目的哪个环节有关？
□ 还有什么不懂的？记录下来明天解决。
```

---

## 里程碑

| 时间点   | 里程碑                        | 验证方法                 |
| -------- | ----------------------------- | ------------------------ |
| 第 7 天  | ✅ 能写 PyTorch 训练循环      | 不看资料写出完整训练代码 |
| 第 11 天 | ✅ 对着摄像头实时输出 AU 值   | MediaPipe 代码跑起来     |
| 第 14 天 | ✅ 能画出论文完整系统框图     | 手绘/Excalidraw          |
| 第 18 天 | ✅ Fusion360 画出一个舵机支架 | 导出 STL 打印            |
| 第 21 天 | ✅ 摄像头→AU→舵机 跑通        | 对着摄像头，舵机动起来   |
| 第 28 天 | ✅ 完整 V1 演示               | 录一个演示视频           |
